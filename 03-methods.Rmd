---
output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/brief_template.tex
documentclass: book
bibliography: references.bib
---

# Methods {#methods}


## Data Cleaning and Transformatoin {#datacleaning}
\textcolor{gray}{by P. Krück}
To examine the given data set properly, the authors first had to restructure and reformat it. This initial data cleaning step included type conversion, value mutation, addition of newly calculated fields and the removal of irrelevant columns.
Concretely, name, category and designer were converted to categorical variables. In the designer column, blank strings and values prefixed by “IKEA of Sweden” were converted to missing values (```NA```). Furthermore, both the price and old price were converted to double values and the currency was changed from Saudi Arabian Riyals to Euros based on the exchange rate from the time the data set was obtained by the author \@ref(#theoretical_background).

Interestingly, the data set had a peculiarity where some rows were exact duplicates except for the category value. The authors considered multiple approaches to handle these data duplications without losing information about the category of an item. 

One considered option was to merge the two category values into one column value via comma separation (e.g. ```"a"``` and ```"b"``` converts to ```"a, b"```). However, this approach leads to the creation of many combinatorial categories with a low count of items per category which also reduces the item count per category where the category isn't comma separated. Overall this would lead to having many small categories which increases the difficulty in applying a regression model due to overfitting \@ref(#overfitting).

The second option was to create separate columns for the different values of ```category```. The data set would then have observations with category one, two and three. While no information is lost utilizing this approach, most observations in the second and third category column would contain missing values, thus increasing the difficulty of analysis using a predefined model @\ref(#random_forest_model).

The authors chose the option of selecting the observations out of the duplicates where the category count occurred most frequently when considering duplicates. The most important categories could be retained without including more column vectors into the data set as in option two.

To better facilitate the comparison of the different sizes of furniture items, the size in cubic meters was computed based on the depth, width and height values, and added as a column vector for further analysis. 
Finally, the authors only selected columns that could have a potential impact on the analysis \@ref(#research_question) for further investigation. A detailed comparison of the initial vs. transformed data structure can be seen in tables \@ref(tab:initial-ikea) and \@ref(tab:tidy-ikea).




TODO: Format these tables
```{r initial-ikea, echo=FALSE}
library('kableExtra')

knitr::kable(head(ikea), caption = "Initial Data Set formatting.")
```

```{r tidy-ikea, echo=FALSE}
knitr::kable(head(tidy_ikea), caption = "Data Set after cleaning process.")
```



## Exploratory Data Analysis {#tbd}
\textcolor{gray}{by P. Krück}
The following sections explore our data based on the eight step data exploration protocol proposed by Zuur et al [@Zuur2010]. 


### Step 1: Outliers in Price and Independent Variables
\textcolor{gray}{by P. Krück}
Outliers of the chosen variables (from tidying see \@ref(#datacleaning) can be observed for each variable (see plot ...). 
The authors assume that outliers do not occur randomly in the form of an observer error. Web scraping code is written in a generic form which makes it generalizable to all applied pages. Thus takes human observation errors out of the equation. Additionally, the authors looked at individual outlier (stichprobenartig) examples and used the provided link column to manually double check observations against machine errors.
By the stated assumption, all outliers are meaningful for further analysis.



### Step 2: Homogeneity of Price
\textcolor{gray}{by P. Krück}
The homogeneity (homoscedasticity) of variance for price is explored by the means of conditional boxplotting. 
Within each name, and within each category the variance is heterogenous (see fig. \@ref(fig:homogeneity)). However, looking at both name and category in conjunction, it is possible to explore homoscedasticity of variance for price.

In the context of this paper, the authors weren't able to inspect all variable combinations for the five categorical variables ($2^3=8$).


```{r, homogeneity, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE, fig.cap="Homogeneity of category for selected combinations of name and category", out.width="100%"}
tidy_ikea %>%
  filter(name == "HEMNES" | name == "LIDHULT" | name == "VIMLE" | name == "VALLENTUNA" | name == "GRÖNLID") %>%
  filter(category == "Beds" | category == "Chairs" | category == "Sofas & armchairs") %>%
  ggplot(mapping = aes(x = reorder(name, price_eur), y = price_eur, color = name)) +
  geom_boxplot(width = 0.4, outlier.size = 0.5, outlier.alpha = 0.3, show.legend = FALSE) + 
  scale_color_manual(values = mycolors) +
  theme_minimal() +
  coord_flip() +
  labs(x = "name", y = "price in Euro", fill = "", subtitle = "category") +
  facet_grid(~ category)
```



### Step 3: Normality 
\textcolor{gray}{by P. Krück}
All numerical variables (```price```, ```old_price``` and ```size_m3```) aren't arranged along a normal distribution (see fig. \@ref(fig:normality)), but rather follow an exponential decay ($e^{-x}$).


```{r, normality, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE, fig.cap="Homogeneity of category for selected combinations of name and category", out.width="100%"}
tidy_ikea %>%
  select(where(is.numeric)) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "num") %>% group_by(variable) %>%
  arrange(num) %>%
  mutate(observation = 1:n()) %>%
  ungroup() %>%
  ggplot(aes(x = num)) + theme_bw() +
  geom_histogram(bins = 100) + facet_wrap(~ variable, scales = "free") + ylab("") + xlab("")
```



### Step 4: Missing Values
\textcolor{gray}{by P. Krück}
All variables were examined for missing values. Only ```designer```, ```size_m3``` and ```old_price_eur``` have missing values of 3.44%, 45.9% and 81% respectively (see fig. \@ref(fig:missing-values))). 
The missing values for designer were deliberately set to ```NA``` by the authors in the case where the values contained digits, which is clearly a scraping error. 
The ```NA``` values for the size can be explained due to the computation of this column vector. ```size_m3``` is the product of ```depth```, ```width``` and ```height```. If one of those three values is missing, the end result is also a missing value. 
The abscence of the old price variables is due to the fact that most items aren't on sale and thus don't have a missing value.

```{r, missing-values, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE, fig.cap="Homogeneity of category for selected combinations of name and category", out.width="100%"}
missing_values <- tidy_ikea %>%
  gather(key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  group_by(key) %>%
  mutate(total = n()) %>%
  group_by(key, total, isna) %>%
  summarise(num.isna = n()) %>%
  mutate(pct = num.isna / total * 100)
## `summarise()` regrouping output by 'key', 'total' (override with `.groups` argument)
levels <-
  (missing_values  %>% filter(isna == T) %>% arrange(desc(pct)))$key

missing_values %>%
  ggplot() +
  geom_bar(aes(x = reorder(key, desc(pct)), 
               y = pct, fill=isna), 
           stat = 'identity', alpha=0.8) +
  scale_x_discrete(limits = levels) +
  scale_fill_manual(name = "", 
                    values = c('steelblue', 'tomato3'), labels = c("Present", "Missing")) +
  coord_flip() +
  labs(title = "Percentage of missing values", x =
         'Variable', y = "% of missing values")
```


### Step 5: Collinearity between Independent Variables
\textcolor{gray}{by P. Krück}
The old price has a rather high VIF which corresponds to high multicollinearity (see table ...). Contrarily, size has a low VIF which translates to low multicollinearity among the other independent variables (see table ...).

TODO: use values from step 5 of eda protocol
```{r initial-ikea2, echo=FALSE}
library('kableExtra')

vif <- c(1, 3, 4)

knitr::kable(vif, caption = "Initial Data Set formatting.")
```



### Step 6: Relationship between Independent Variables and Price {#relationship}
Inspecting the relationship between the independent variables and price strong relationships between ```old_price_eur``` and ```size_m3``` can be observed, while no relationship can be observed for the other variables (see \@ref(fig:relationship-x-y)).
```old_price_eur``` has a linear relationship (see fig. \@ref(fig:relationship-old-price)) whereas ```size_m3``` fits a second order polynomial (see fig. \@ref(fig:relationship-size-m3)) to price.

```{r relationship-old-price, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE, fig.cap="caption", out.width="100%"}
tidy_ikea %>%
  filter(!is.na(old_price_eur)) %>%
  ggplot(aes(x = old_price_eur, y = price_eur)) +
    geom_point(color = mycolors[1], alpha = 0.5) +
    geom_smooth(color = mycolors[5], se = FALSE) +
    theme_minimal() +
    labs(x = "old price in €", y = "price in €", title = "Price vs. Old Price")
```

```{r relationship-size-m3, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE, fig.cap="caption", out.width="100%"}
tidy_ikea %>%
  filter(!is.na(size_m3)) %>%
  ggplot(aes(x = size_m3, y = price_eur, colour = "red")) +
    geom_point(alpha = 0.3, show.legend = FALSE) +
    geom_smooth(show.legend = FALSE, color = "orange", fill = "orange", alpha = 0.25) +
    scale_color_manual(values = mycolors) +
    theme_minimal() +
    labs(x = "size in m^3", y = "price in €", title = "Price by Volume")
```

### Step 7: Interactions
The interactions between different variables is explored by the means of coplotting.
Using this form plotting the relationship of two numerical variables is explored by creating a matrix of plots subdivided by two categorical variables. 
In the given data set there are three numerical and three categorical variables which can be explored in this form of interaction. For the numerical variables, ```old_price_eur``` has such a strong relationship with price (see ...) that a more detailed breakdown by the categorical variables wouldn't reveal new information. This leaves the exploration of ```size_m3``` and ```price_eur``` broken down by designer, name and category resulting in (3 out of 2 = 3) combinations of coplots.
Unfortunately, the authors of this papers weren't able to fully explore all combinations due to programming difficulties.
Plotting designer and name works ....

- Coplotting designer and name works, while the two combination would not plot
- The linear model predicted infinite values and thus coplot the values properly for the other two options
- However, dropping all NA values and thus reducing the total data size to 354 observations would case for the combination coplot name and category while name + designer combination would not work
- The authors hypothesized that infinite values were caused by a division of zeros of the linear model since there occured 0 values in size
- This however proved to be wrong after applying respective filters

- The following interaction could be analyzed
- - There is probably no significant interaction between size, price, name & designer as can be seen in coplot -> lines are nearly parallel
- Based on the coplot of category and name with the 354 observations, inparallelity could be observed and thus could conclude a interaction. However, this could also be due to the small sample size 
-  


(footnote): the authors would highly appreciate any solutions on this matter
^[This is a footnote.]



### Step 8: Independence of Price
- Durch das tidying in (cross reference) duplicate removal -> Zuur paper Step 8 1. citation: meaning that information from any one observation should not provide information on another after the effects of other variables have been accounted for. This concept is best explained with examples.


## Random Forest Regression Model {#rf}
\textcolor{gray}{by J. Pein}

TODO: Students  should decide  on  an  appropriate  statistical  procedure  to answer  their chosen research questionand should state any prerequisites /assumptions of this methodaccordingly.


This analysis was conducted using the R randomForest package, which is based on the original Breiman and Cutler's Fortran code for random forest regression. To learn more about how random forests work or the randomForest package, see @Liaw2002 or #chapter2 #TODO. To reproduce the analysis conducted in this paper, the prepatory steps are described here. The following steps are based on an already cleaned ikea data frame which is described in \@ref(datacleaning). This data frame is then transformed further to be used with the randomForest package.

First, the variable `old_price_eur` is removed from the data frame, due to a very high correlation and relationship to the price variable analyzed in \@ref(collinearity) and \@ref(relationship) Then, the designers and names, which are not part of the 50 `designers` and 49 `names` with the highest number of occurences, are grouped in the `other` value. This is because the `randomForest` method does not allow categorical variables with more than 53 predictors. The last step is dealing with the missing values in the data. As described in \@ref(zeros), there are missing values in the `size_m3` and `designer` variables. To use the `randomForest` method of the randomForest package on the data, those missing values are dealt with using three different approaches. In the first approach the rows with missing values are deleted, reducing the total number of rows by aproximately 50%. In the second approach the missing values are dummy coded with a value of -1000. The third approach uses the `na.roughfix = na.omit` argument, which is the built in way of the randomForest package to deal with missing values.
After preparing the data, the `randomForest` method of the randomForest package is applied to the data with number of trees set to 2000 and importance set to `TRUE`.

`randomForest(price_eur ~ ., rf_ikea, ntree=2000, keep.forest=FALSE, importance=TRUE)`

Then the `importance` method of the randomForest package is used to save the feature importances, which are computed by permuting feature importance, which is described in #chapter2 #TODO. The three different approaches of dealing with the missing values in the data set lead to different results, so the authors chose to calculate the mean result of the three approaches. The result of this analysis is presented in the following chapter.

















