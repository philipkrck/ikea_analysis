%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% OXFORD THESIS TEMPLATE

% Use this template to produce a standard thesis that meets the Oxford University requirements for DPhil submission
%
% Originally by Keith A. Gillow (gillow@maths.ox.ac.uk), 1997
% Modified by Sam Evans (sam@samuelevansresearch.org), 2007
% Modified by John McManigle (john@oxfordechoes.com), 2015
% Modified by Ulrik Lyngs (ulrik.lyngs@cs.ox.ac.uk), 2018, for use with R Markdown
%
% Ulrik Lyngs, 25 Nov 2018: Following John McManigle, broad permissions are granted to use, modify, and distribute this software
% as specified in the MIT License included in this distribution's LICENSE file.
%
% John tried to comment this file extensively, so read through it to see how to use the various options.  Remember
% that in LaTeX, any line starting with a % is NOT executed.  Several places below, you have a choice of which line to use
% out of multiple options (eg draft vs final, for PDF vs for binding, etc.)  When you pick one, add a % to the beginning of
% the lines you don't want.


%%%%% CHOOSE PAGE LAYOUT
% The most common choices should be below.  You can also do other things, like replacing "a4paper" with "letterpaper", etc.

% This one will format for two-sided binding (ie left and right pages have mirror margins; blank pages inserted where needed):
%\documentclass[a4paper,twoside]{templates/ociamthesis}
% This one will format for one-sided binding (ie left margin > right margin; no extra blank pages):
%\documentclass[a4paper]{ociamthesis}
% This one will format for PDF output (ie equal margins, no extra blank pages):
%\documentclass[a4paper,nobind]{templates/ociamthesis}
%UL 2 Dec 2018: pass this in from YAML
\documentclass[a4paper, nobind]{templates/ociamthesis}


% UL 30 Nov 2018 pandoc puts lists in 'tightlist' command when no space between bullet points in Rmd file
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
 
% UL 1 Dec 2018, fix to include code in shaded environments
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
%UL 2 Dec 2018 add a bit of white space before and after code blocks
\renewenvironment{Shaded}
{
  \vspace{4pt}%
  \begin{snugshade}%
}{%
  \end{snugshade}%
  \vspace{4pt}%
}

%UL 2 Dec 2018 reduce whitespace around verbatim environments
\usepackage{etoolbox}
\makeatletter
\preto{\@verbatim}{\topsep=0pt \partopsep=0pt }
\makeatother

%UL 26 Mar 2019, enable strikethrough
\usepackage[normalem]{ulem}

%UL 15 Oct 2019, enable link highlighting to be turned off from YAML
\usepackage[colorlinks=false,pdfpagelabels,hidelinks=true]{hyperref}

%%%%% SELECT YOUR DRAFT OPTIONS
% Three options going on here; use in any combination.  But remember to turn the first two off before
% generating a PDF to send to the printer!

% This adds a "DRAFT" footer to every normal page.  (The first page of each chapter is not a "normal" page.)

% This highlights (in blue) corrections marked with (for words) \mccorrect{blah} or (for whole
% paragraphs) \begin{mccorrection} . . . \end{mccorrection}.  This can be useful for sending a PDF of
% your corrected thesis to your examiners for review.  Turn it off, and the blue disappears.
\correctionstrue

%%%%% BIBLIOGRAPHY SETUP
% Note that your bibliography will require some tweaking depending on your department, preferred format, etc.
% The options included below are just very basic "sciencey" and "humanitiesey" options to get started.
% If you've not used LaTeX before, I recommend reading a little about biblatex/biber and getting started with it.
% If you're already a LaTeX pro and are used to natbib or something, modify as necessary.
% Either way, you'll have to choose and configure an appropriate bibliography format...

% The science-type option: numerical in-text citation with references in order of appearance.
% \usepackage[style=numeric-comp, sorting=none, backend=biber, doi=false, isbn=false]{biblatex}
% \newcommand*{\bibtitle}{References}

% The humanities-type option: author-year in-text citation with an alphabetical works cited.
% \usepackage[style=authoryear, sorting=nyt, backend=biber, maxcitenames=2, useprefix, doi=false, isbn=false]{biblatex}
% \newcommand*{\bibtitle}{Works Cited}

%UL 3 Dec 2018: set this from YAML in index.Rmd
\usepackage[style=authoryear, sorting=nyt, backend=biber, maxcitenames=2, useprefix, doi=true, isbn=false, uniquename=false]{biblatex}
\newcommand*{\bibtitle}{Works Cited}

% This makes the bibliography left-aligned (not 'justified') and slightly smaller font.
\renewcommand*{\bibfont}{\raggedright\small}

% Change this to the name of your .bib file (usually exported from a citation manager like Zotero or EndNote).
\addbibresource{references.bib}


% Uncomment this if you want equation numbers per section (2.3.12), instead of per chapter (2.18):
%\numberwithin{equation}{subsection}


%%%%% THESIS / TITLE PAGE INFORMATION
% Everybody needs to complete the following:
\title{Analyzing the Feature Importance of Different Variables on the Price of Ikea Products}
\author{Philip Krück, Johannes Pein}
\college{}

% Master's candidates who require the alternate title page (with candidate number and word count)
% must also un-comment and complete the following three lines:
%\masterssubmissiontrue
%\candidateno{933516}
%\wordcount{28,815}

% Uncomment the following line if your degree also includes exams (eg most masters):
%\renewcommand{\submittedtext}{Submitted in partial completion of the}
% Your full degree name.  (But remember that DPhils aren't "in" anything.  They're just DPhils.)
\degree{B.Sc. Business Informatics (18A-BI)}
% Term and year of submission, or date if your board requires (eg most masters)
\degreedate{04.12.2020}


%%%%% YOUR OWN PERSONAL MACROS
% This is a good place to dump your own LaTeX macros as they come up.
\modulename{Digital Toolbox: Data Business}
\lecturer{Lecturer: Ulf Köther}
\groupnumber{Group Number: 7}
\matriculationnumbers{Matriculation Numbers: 3938 (P.Krück), 4001 (J.Pein)}


% To make text superscripts shortcuts
	\renewcommand{\th}{\textsuperscript{th}} % ex: I won 4\th place
	\newcommand{\nd}{\textsuperscript{nd}}
	\renewcommand{\st}{\textsuperscript{st}}
	\newcommand{\rd}{\textsuperscript{rd}}

%%%%% THE ACTUAL DOCUMENT STARTS HERE
\begin{document}

%%%%% CHOOSE YOUR LINE SPACING HERE
% This is the official option.  Use it for your submission copy and library copy:
\setlength{\textbaselineskip}{22pt plus2pt}
% This is closer spacing (about 1.5-spaced) that you might prefer for your personal copies:
%\setlength{\textbaselineskip}{18pt plus2pt minus1pt}

% You can set the spacing here for the roman-numbered pages (acknowledgements, table of contents, etc.)
\setlength{\frontmatterbaselineskip}{17pt plus1pt minus1pt}


% UL: You can set the general paragraph spacing here - I've set it to 2pt (was 0) so
% it's less claustrophobic
\setlength{\parskip}{2pt plus 1pt}


% Leave this line alone; it gets things started for the real document.
\setlength{\baselineskip}{\textbaselineskip}


%%%%% CHOOSE YOUR SECTION NUMBERING DEPTH HERE
% You have two choices.  First, how far down are sections numbered?  (Below that, they're named but
% don't get numbers.)  Second, what level of section appears in the table of contents?  These don't have
% to match: you can have numbered sections that don't show up in the ToC, or unnumbered sections that
% do.  Throughout, 0 = chapter; 1 = section; 2 = subsection; 3 = subsubsection, 4 = paragraph...

% The level that gets a number:
\setcounter{secnumdepth}{2}
% The level that shows up in the ToC:
\setcounter{tocdepth}{2}


% JEM: Pages are roman numbered from here, though page numbers are invisible until ToC.  This is in
% keeping with most typesetting conventions.
\begin{romanpages}

% Title page is created here
\maketitle

%%%%% MINI TABLES
% This lays the groundwork for per-chapter, mini tables of contents.  Comment the following line
% (and remove \minitoc from the chapter files) if you don't want this.  Un-comment either of the
% next two lines if you want a per-chapter list of figures or tables.
  \dominitoc % include a mini table of contents

% This aligns the bottom of the text of each page.  It generally makes things look better.
\flushbottom

% This is where the whole-document ToC appears:
\tableofcontents

%%%%% LIST OF ABBREVIATIONS
% This example includes a list of abbreviations.  Look at text/abbreviations.tex to see how that file is
% formatted.  The template can handle any kind of list though, so this might be a good place for a
% glossary, etc.
\include{front-and-back-matter/abbreviations}

% The Roman pages, like the Roman Empire, must come to its inevitable close.
\end{romanpages}

%%%%% CHAPTERS
% Add or remove any chapters you'd like here, by file name (excluding '.tex'):
\flushbottom

% all your chapters and appendices will appear here
\hypertarget{intro}{%
\chapter{Introduction}\label{intro}}

This project report is an examination at the Hamburg School of Business Administration in the module `Data Business' as a part of a Bachelor of Science degree program. The students were given a data set and the task was to first explore the data and then choose a research question which was to be answered scientifically with the help of the statistical programming language R. The authors of this report were given a data set which contains Ikea products with different features, such as price, dimensional measures, name, category and designers of the product.

\hypertarget{chapter-2}{%
\chapter{Theoretical Background \& Research Question}\label{chapter-2}}

\hypertarget{data-set}{%
\section{Data Set}\label{data-set}}

\textcolor{gray}{by P. Krück}

The data set was obtained by a kaggle.com user (Reem Abdulrahman) by the means of webscraping techniques from the Saudi Arabian Ikea website in the furniture category on the 20th of April 2020. Noteworthy features include the name, category, price in Saudi Riyals, the designer and dimensions (width, height and depth).
The data set has 13 variables and 2962 distinct observations after the removal of duplicates.

\hypertarget{theoretical-background}{%
\section{Theoretical Background}\label{theoretical-background}}

\hypertarget{random-forest-basics}{%
\subsection{Random Forest Basics}\label{random-forest-basics}}

\textcolor{gray}{by J. Pein}

In order to analyze the feature importance in relation to the price variable, a random forest regression model was chosen. A random forest consists of many decision trees, which predicts the response variable based on a majority decision process. In standard decision trees, each node is split to achieve the best performing model. In random forests however, the nodes are randomly split.
Compared to linear regression, random forests not only take the mean and covariance structure into account, but also include deeper aspects of the data\footnote{Grömping, ``Variable Importance Assessment in Regression: Linear Regression versus Random Forest.''} resulting in more advanced and robust model. To learn more about random forests, please see Breiman.\footnote{``Random Forests.''}

\hypertarget{overfitting}{%
\subsection{Overfitting}\label{overfitting}}

\textcolor{gray}{by P. Krück}

In statistical modelling, overfitting refers to the phenomenon where an analysis model corresponds to closely to a given data set and thus fails to generalize to new data or future observations.

\hypertarget{feature-importance}{%
\subsection{Feature Importance}\label{feature-importance}}

\textcolor{gray}{by J. Pein}

There are different ways to measure feature importance. In this analysis permuting feature importance by a random forest algorithm is used. This algorithm leaves each feature out once while leaving all others unchanged, at each step calculating the mean squared error (MSE) of the predictions. This is done for each tree, calculating the overall MSE of each feature for the whole model.\footnote{Ibid.}

\hypertarget{research-question}{%
\section{Research Question}\label{research-question}}

\textcolor{gray}{by P. Krück}

This paper explores the following research question:

\emph{How important are the different features of Ikea products in regard to their price?}

The motivating forces for this research question are the possible implications for price determination of new items.

\hypertarget{methods}{%
\chapter{Methods}\label{methods}}

\hypertarget{datacleaning}{%
\section{Data Cleaning and Transformation}\label{datacleaning}}

\textcolor{gray}{by P. Krück}

To examine the given data set properly, the authors first had to restructure and reformat it. This initial data cleaning step included type conversion, value mutation, addition of newly calculated fields and the removal of irrelevant columns.

Concretely, \texttt{name}, \texttt{category} and \texttt{designer} were converted to categorical variables. In the \texttt{designer} column, blank strings and values prefixed by ``IKEA of Sweden'' were converted to missing values (\texttt{NA}). Furthermore, both the price and old price were converted to double values and the currency was changed from Saudi Arabian Riyals to Euros based on the exchange rate from the time the data set was obtained by the author (See section \ref{theoretical-background}).

Interestingly, the data set had a peculiarity where some rows were exact duplicates except for values in the \texttt{category} vector. The authors considered multiple approaches to handle these data duplications without losing information about the category of an item.

One considered option was to merge the two category values into one column value via comma separation (e.g. \texttt{"a"} and \texttt{"b"} converts to \texttt{"a,\ b"}). However, this approach leads to the creation of many combinatorial categories with a low count of items per category.
Additionally, it reduces the item count per category where the category isn't comma separated.
Overall this would lead to having many small categories which increases the difficulty in applying a regression model due to overfitting (See section \ref{overfitting}).

The second option was to create separate columns for the different values of \texttt{category}. The data set would then have observations with category one, two and three. While no information is lost utilizing this approach, most observations in the second and third category column would contain missing values, thus increasing the difficulty of analysis using a predefined model (See section \ref{rf}).

The authors chose the option of selecting the observations where the category count occurred most frequently when considering duplicates. The most important categories could be retained without including more column vectors into the data set as in option two.

To better facilitate the comparison of the different sizes of furniture items, the size in cubic meters was computed based on the depth, width and height values, and added as a column vector for further analysis.

Finally, the authors only selected columns that could have a potential impact on outcome of the analysis (See section \ref{research-question}) for further investigation. A detailed comparison of the initial vs.~transformed data structure can be seen in tables \ref{tab:initial-ikea} and \ref{tab:tidy-ikea}.

\begin{table}

\caption{\label{tab:initial-ikea}Initial Data Set formatting.}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{l|l|l|l|l|l|l|l}
\hline
name & category & price & old\_price & sellable\_online & other\_colors & designer & ...\\
\hline
PAX... & Wardrobes & 20450 & No old price & TRUE & No & IKEA ... & ...\\
\hline
ELV... & Wardrobes & 7500 & SR 820 & TRUE & No & Ehlén... & ...\\
\hline
ELV... & Wardrobes & 15720 & SR 1,755 & TRUE & No & Ehlén... & ...\\
\hline
ELV... & Wardrobes & 9240 & SR 1,050 & TRUE & No & Ehlén... & ...\\
\hline
ELV... & Wardrobes & 27450 & SR 3,130 & TRUE & No & Ehlén... & ...\\
\hline
ELV... & Wardrobes & 12310 & SR 1,535 & TRUE & No & Ehlén... & ...\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:tidy-ikea}Transformed Data Set formatting.}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{l|l|l|l|l|l|l|l}
\hline
name & category & price\_eur & old\_price\_eur & sellable\_online & other\_colors & designer & size\_m3\\
\hline
PAX... & Wardrobes & 501.78 & NA & TRUE & FALSE & Ehlén... & 3.12\\
\hline
ELV... & Wardrobes & 184.03 & 201.2 & TRUE & FALSE & Ehlén... & NA\\
\hline
ELV... & Wardrobes & 385.72 & 430.62 & TRUE & FALSE & Ehlén... & NA\\
\hline
ELV... & Wardrobes & 226.72 & 257.64 & TRUE & FALSE & Ehlén... & NA\\
\hline
ELV... & Wardrobes & 673.54 & 768.01 & TRUE & FALSE & Ehlén... & NA\\
\hline
ELV... & Wardrobes & 302.05 & 376.64 & TRUE & FALSE & Ehlén... & NA\\
\hline
\end{tabular}
\end{table}

\hypertarget{tbd}{%
\section{Exploratory Data Analysis}\label{tbd}}

\textcolor{gray}{by P. Krück}

The following sections explore our data based on the eight step data exploration protocol proposed by Zuur et al.\footnote{Zuur, ``A protocol for data exploration to avoid common statistical problems.''}

\hypertarget{outliers}{%
\subsection{Step 1: Outliers in Price and Independent Variables}\label{outliers}}

\textcolor{gray}{by P. Krück}

Outliers of the chosen variables can be observed for each variable (see figure \ref{fig:outliers}).
Web scraping code is written in a generic form which makes it generalizable to all applied pages. This removes the occurance of human observation errors in the Ikea data set. Additionally, it is also unlikely that the outliers are due to falsely scraped items.
To verify this hypothesis, the authors randomly tested outlier observations and manually checked them against the ikea website information. With a high level of statistical confidence, the authors proceeded with further analysis without removing outliers as observation errors.

\begin{figure}
\includegraphics[width=1\linewidth]{_main_files/figure-latex/outliers-1} \caption{Boxplots for Price in € based on Independent Variables}\label{fig:outliers}
\end{figure}

\hypertarget{step-2-homogeneity-of-price}{%
\subsection{Step 2: Homogeneity of Price}\label{step-2-homogeneity-of-price}}

\textcolor{gray}{by P. Krück}

The homogeneity (homoscedasticity) of variance for price is explored by the means of conditional boxplotting.
Within each name, and within each category the variance is heterogenous (see fig. \ref{fig:homogeneity}). However, looking at both name and category in conjunction, it is possible to explore homoscedasticity of variance for price.

Due to the limited scope and length of this paper, the authors were not able to inspect all variable combinations for the three categorical plus two logical variables (\(2^5=32\)).

\begin{figure}
\includegraphics[width=1\linewidth]{_main_files/figure-latex/homogeneity-1} \caption{Homogeneity of category for selected combinations of name and category}\label{fig:homogeneity}
\end{figure}

\hypertarget{step-3-normality}{%
\subsection{Step 3: Normality}\label{step-3-normality}}

\textcolor{gray}{by P. Krück}

All numerical variables (\texttt{price}, \texttt{old\_price} and \texttt{size\_m3}) aren't arranged along a normal distribution (see fig. \ref{fig:normality}), but rather follow an exponential decay (\(e^{-x}\)).

\begin{figure}
\includegraphics[width=1\linewidth]{_main_files/figure-latex/normality-1} \caption{Histogram of Numerical Variables with a Bin Width of 100 Euro}\label{fig:normality}
\end{figure}

\hypertarget{missing-values}{%
\subsection{Step 4: Missing Values}\label{missing-values}}

\textcolor{gray}{by P. Krück}

All variables were examined for missing values. Only \texttt{designer}, \texttt{size\_m3} and \texttt{old\_price\_eur} have missing values with percentages of 3.44\%, 45.9\% and 81\% respectively (see fig. \ref{fig:missing-values}).
The missing values for designer were deliberately set to \texttt{NA} by the authors in the case where the values contained digits, which is clearly a scraping error.
The \texttt{NA} values for the size can be explained due to the computation of this column vector. \texttt{size\_m3} is the product of \texttt{depth}, \texttt{width} and \texttt{height}. If one of those three values is missing, the end result is also a missing value.
In contrast, the abscence of the old price variables is due to the fact that most items aren't on sale and thus don't have a missing value.

\begin{figure}
\includegraphics[width=1\linewidth]{_main_files/figure-latex/missing-values-1} \caption{Percentage of missing values}\label{fig:missing-values}
\end{figure}

\hypertarget{collinearity}{%
\subsection{Step 5: Collinearity between Independent Variables}\label{collinearity}}

\textcolor{gray}{by P. Krück}

The old price has a rather high VIF which corresponds to high multicollinearity (see table \ref{tab:vif}). Contrarily, size has a low VIF which translates to low multicollinearity among the other independent variables (see table \ref{tab:vif}).

\begin{table}

\caption{\label{tab:vif}Variance Inflation Factors for Numerical Variables}
\centering
\begin{tabular}[t]{r|r|r}
\hline
price\_eur & old\_price\_eur & size\_m3\\
\hline
77.97 & 78.54 & 2.36\\
\hline
\end{tabular}
\end{table}

\hypertarget{relationship}{%
\subsection{Step 6: Relationship between Independent Variables and Price}\label{relationship}}

\textcolor{gray}{by P. Krück}

Inspecting the relationship between the independent variables and price, a strong correlation between \texttt{old\_price\_eur} and \texttt{size\_m3} can be observed, while none can be detected for the other variables (see \ref{fig:relationship-x-y}).
\texttt{old\_price\_eur} has a linear relationship (see fig. \ref{fig:relationship-old-price}) whereas \texttt{size\_m3} fits a second order polynomial to price (see fig. \ref{fig:relationship-size-m3}).

\begin{figure}
\includegraphics[width=1\linewidth]{_main_files/figure-latex/relationship-old-price-1} \caption{Relationshihp of Price and Old Price}\label{fig:relationship-old-price}
\end{figure}

\begin{figure}
\includegraphics[width=1\linewidth]{_main_files/figure-latex/relationship-size-m3-1} \caption{Relationship of Price Size in Cubic Meters}\label{fig:relationship-size-m3}
\end{figure}

\hypertarget{step-7-interactions}{%
\subsection{Step 7: Interactions}\label{step-7-interactions}}

\textcolor{gray}{by P. Krück}

The interactions between different variables is explored by the use of conditioning plots (coplots).
Using this form plotting the relationship of two numerical variables is explored by creating a matrix of plots subdivided by two categorical variables.
In the given data set there are three numerical and three categorical variables which can be explored in this form of interaction. For the numerical variables, \texttt{old\_price\_eur} has such a strong relationship with \texttt{price} (see fig \ref{fig:relationship-old-price}). A more detailed breakdown by the categorical variables wouldn't reveal new information. This leaves the exploration of \texttt{size\_m3} and \texttt{price\_eur} subdivided by designer, name and category resulting in (\({3 \choose 2} = 3\)) combinations of coplots.

\hypertarget{size-price-interaction}{%
\subsubsection{Interaction of size and price coplotted by designer and name}\label{size-price-interaction}}

\textcolor{gray}{by P. Krück}

It is unlikely that there is an interaction between size and price split by name and designer as is indicated by the non-parallelism of the fitted lines in the coplot (see figure \ref{asdf})

\hypertarget{problems-with-coplot-development}{%
\subsubsection{Problems with Coplot Development}\label{problems-with-coplot-development}}

\textcolor{gray}{by P. Krück}

This section describes a programming error the authors ran into regarding coplotting.

Unfortunately, the authors of this papers weren't able to fully explore all combinations.
Plotting designer and name works (see section \ref{size-price-interaction}) while the other two options would not plot properly.
The authors could not fully debug the problem with these plots.
The linear model predicted infinite values for some of the coplotted combinations for both amalgamations that wouldn't render correctly.
Dropping all \texttt{NA} values left 354 observations and the coplot would correctly render for the combination of \texttt{name} and \texttt{category} while for \texttt{name} and \texttt{designer} it would not. The number of observations for \texttt{name} and \texttt{category} is rather low considering the additional categorical subdivision which lead the authors to discard it as an insignificant research finding.
Still there seemed to be infinite values outputted by the linear method. The authors hypothesized that those values were caused by a division by 0 of the internal algorithm mechanics. This however proved to be wrong after applying the respective filters.
The code for the 3 plots can be viewed in Appendix section \ref{coplot-category-name} and \ref{coplot-designer-category}.

The inclined reader is encouraged to dabble with the code. Sending any hints or even a solution to fix the code and fully render the plots would be highly appreciated by the authors.\footnote{Any questions, hints or solutions may kindly be sent to philip.krueck@myhsba.de.}

\hypertarget{step-8-independence-observations-of-response-variable-price}{%
\subsection{Step 8: Independence Observations of Response Variable Price}\label{step-8-independence-observations-of-response-variable-price}}

\textcolor{gray}{by P. Krück}

The independence of observerations of the response variables assumes that ``{[}\ldots{}{]} information from any one observation should not provide information on another after the effects of other variables have been accounted for.''\footnote{Ibid., 11, ll.23--26.}
The data cleaning step left the observed data set in a tidy format which implies that the observations are independent of eachother.\footnote{``There are three interrelated rules which make a dataset tidy: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell.'' (, Wickham, \emph{R for Data Science}, 149, ll.4--7.)}

\hypertarget{rf}{%
\section{Random Forest Regression Model}\label{rf}}

\textcolor{gray}{by J. Pein}

This analysis was conducted using the R \emph{randomForest} package, which is based on the original Breiman and Cutler's Fortran code for random forest regression. To learn more about how random forests work see chapter \ref{chapter-2}, to learn more about the \emph{randomForest} package see Liaw and Wiener.\footnote{``Classification and Regression by randomForest.''} To reproduce the analysis conducted in this paper, the prepatory steps are described here. These steps are based on the cleaned ikea data set which is described in section \ref{datacleaning}. This data set is then transformed further allowing it to be used with the \emph{randomForest} package.

First, the variable \texttt{old\_price\_eur} is removed from the cleaned ikea data set, due to a very high correlation and relationship to the response variable \texttt{price\_eur} analyzed in section \ref{collinearity} and section \ref{relationship}. Then, the \texttt{designers} and \texttt{names}, which are not part of the 50 \texttt{designers} and 49 \texttt{names} with the highest number of occurences, are grouped in the \texttt{other} value. This is because the \texttt{randomForest} method does not allow categorical variables with more than 53 predictors. The last step deals with the missing values in the data. As described in section \ref{missing-values}, there are many missing values in the \texttt{size\_m3} and \texttt{designer} variables. To apply the \texttt{randomForest} method of the \emph{randomForest} package on the data, those missing values are treated using three different approaches. In the first approach the rows with missing values are deleted, reducing the total number of rows by aproximately 50\%. In the second approach the missing values are dummy coded with a value of -1000. The third approach uses the \texttt{na.roughfix\ =\ na.omit} argument, which is the built-in way of the \emph{randomForest} package to deal with missing values.
After preparing the data, the \texttt{randomForest} method of the \emph{randomForest} package is applied to the data with number of trees set to 2000 and importance set to \texttt{TRUE}, training the random forest model with the cleaned ikea data set.

\texttt{randomForest(price\_eur\ \textasciitilde{}\ .,\ rf\_ikea,\ ntree=2000,\ importance=TRUE)}

Then the \texttt{importance} method of the \emph{randomForest} package is used to calculate the feature importances, which are computed by permuting feature importance, which was introduced in section \ref{feature-importance}. The three different approaches of dealing with the missing values in the data set lead to different results, so the authors chose to calculate the mean result of the three approaches. The result of this analysis is presented in the following chapter.

\hypertarget{results}{%
\chapter{Results}\label{results}}

\hfill\textcolor{gray}{by J. Pein}

\hypertarget{todo-set-n_trees-to-2000-before-handing-in}{%
\section{TODO: set n\_trees to 2000 before handing in}\label{todo-set-n_trees-to-2000-before-handing-in}}

In this chapter, the result of the analysis of the feature importance of different features on the response variable price of Ikea products are presented.

As described in section \ref{rf}, the feature importance was calculated using permuting feature importance of the \emph{randomForest} R package. In this analysis, feature importance is derived from the percentage increase of the mean squared error (MSE) of the overall random forest regression model in regard to the response variable \texttt{price\_eur}. A larger percentage increase of the MSE implies greater feature importance. Conversely, a lower percentage increase of the MSE translates to less significant feature importance.

\begin{figure}
\includegraphics[width=1\linewidth]{_main_files/figure-latex/mean-feature-importance-1} \caption{A plot showing the mean feature importance of the predictor variables on the response variable}\label{fig:mean-feature-importance}
\end{figure}

Thus, as can be seen in figure \ref{fig:mean-feature-importance}, the most important feature is \texttt{size\_m3} with an increase of the MSE of 182\%. The second, third and fourth most important features are \texttt{designer} with an increase of 120\%, \texttt{name} with an increase of 114\% and \texttt{category} with an increase of 105\%. The fifth most important feature is \texttt{other\_colors} with a MSE increase of 78\% and the least important feature is \texttt{sellable\_online} with a 9\% increase.

These results are further discussed in the following chapter.

\hypertarget{discussion}{%
\chapter{Discussion}\label{discussion}}

\hfill\textcolor{gray}{by J. Pein}

In this chapter, the results are discussed in connection with the research question. The question that the results were supposed to answer is the following:

\emph{How important are the different features of Ikea products in regard to their price?}

\hypertarget{feature-importance-1}{%
\section{Feature Importance}\label{feature-importance-1}}

The \texttt{size\_m3} variable is the most important feature. Probably the main reason for this is the size of a product being closely linked to its material cost. Big items are generally more costly to produce, thus leading to a higher selling price and vice versa. Due to the high correlation to the price variable described in section \ref{collinearity} and section \ref{relationship}, it is worth discussing whether or not to include this variable in a possible predictive analysis model in future research.

The \texttt{designer} variable is the second most important feature. It might seem, that this is due to \emph{overfitting}, since the random forest regression model takes into account around 50 different combinations of designers, partly with a low number of occurences. But according to Breiman\footnote{``Random Forests,'' 29.} random forest models are robust against overfitting. Further research should be conducted to analyze whether overfitting is present or not.
When looking at the price distribution per designer, in can be clearly seen that the interquartile range (IQR) of price varies for each designer. In addition, the IQR often is smaller than 300€, thus showing a tendency towards a certain price range, which might be the reason for the relatively high feature importance of the designer variable. For the plot, see figure \ref{fig:price-dist-per-designer}.

The data set, which the random forest was trained on, includes around 50 different product names with partly small numbers of occurences. Thus, the relatively high feature importance of the \texttt{name} variable might also be caused by overfitting. As discussed above, further research should be conducted to analyze whether this is the case. On the other hand, as can be seen in figure \ref{fig:homogeneity}, certain product lines (names) tend to be more expensive than others. \emph{LIDTHULT}, for example, is the most expensive product line in each of the categories \emph{beds}, \emph{chairs} and \emph{sofas \& armchairs} while \emph{HEMNES} is on the lower end of the price scale. This behavior explains a relatively high feature importance of the \texttt{name} variable.

\texttt{Category} is another feature with a relatively high importance. This is because the different category's price distributions show a clear tendency towards certain price segments (see figure \ref{fig:price-dist-per-category}), i.e.~Wardrobes and beds are generally more expensive than chairs.
To the authors it seems counterintuitive that \texttt{designer} and \texttt{name} have higher feature importances than the \texttt{category} feature, because the IQR in the price distribution per category is often a lot smaller than the IQR of the designer and name price distributions. This also hints towards overfitting of the name and designer variables. However, in the categories with the most occurrences, namely \texttt{Wardrobes}, \texttt{Sofas\ \&\ armchairs} and \texttt{Beds}, the IQR is relatively large and there are many overlapping prices ranges for different categories, which explains a lower feature importance.

The feature importance of \texttt{other\_colors} is the second lowest, but still considerable. This still relatively high feature importance might be due to the difference of the mean price and the relatively small IQR (see: figure \ref{fig:price-dist-other-colors}). On the other hand, there is a large overlapping area within the IQR in the two expressions of \texttt{other\_colors} possibly reducing the feature importance.

The very low feature importance of the \texttt{sellable\_online} variable is probably because the low number of occurences of a product being sellable online. Only around 0.6\% of the products are sellable online.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

The authors were surprised of the high feature importances of the variables \texttt{designer} and \texttt{name} computed by the random forest model. The thesis of this being due to overfitting is objected by scientific research.\footnote{Ibid., 29.} If the high feature importance of the variables truly is not due to overfitting, this example backs up the thesis of Grömping\footnote{``Variable Importance Assessment in Regression: Linear Regression versus Random Forest,'' 317.} that random forests are able to discover deeper patterns in the data. These patterns are beyond the boxplot discussion applied here, showing the power of random forests models.

Also, the data was scraped from the Saudi Arabian Ikea website, thus this analysis mainly focusses on Ikea products in the Saudi Arabian market. To analyze the geographically independent feature importances, more data should be scraped from other international Ikea websites. The research question of this paper, \emph{How important are the different features of Ikea products in regard to their price?}, could thus not be answered for the global Ikea product market, but for the Saudi Arabian market only. Also, in further research, the results presented on the feature importance of the predictor variables on the response variable price should be validated by other techniques than the random forest method used in this analysis to get unbiased results.

Furthermore, based on this analysis a predictive model could be developed which predicts the price of Ikea products in the Saudi Arabian Market based on the features analyzed. This could be used by Ikea internally to analyze if the price of their new product aligns with the prices of the currently available product or by market researchers.

\startappendices

\hypertarget{appendix}{%
\chapter{Appendix}\label{appendix}}

\begin{figure}
\includegraphics[width=1\linewidth]{_main_files/figure-latex/price-dist-per-designer-1} \caption{A Plot Showing the Price Distribution per Designer}\label{fig:price-dist-per-designer}
\end{figure}

\begin{figure}
\includegraphics[width=1\linewidth]{_main_files/figure-latex/price-dist-per-category-1} \caption{A Plot Showing the Price Distribution per Category}\label{fig:price-dist-per-category}
\end{figure}

\begin{figure}
\includegraphics[width=1\linewidth]{_main_files/figure-latex/price-dist-other-colors-1} \caption{A Plot Showing the Price Distribution Other Colors}\label{fig:price-dist-other-colors}
\end{figure}

\begin{figure}[!h]
\includegraphics[width=1\linewidth]{_main_files/figure-latex/price-dist-sellable-online-1} \caption{A Plot Showing the Price Distribution Sellable Online}\label{fig:price-dist-sellable-online}
\end{figure}

\begin{figure}[!h]
\includegraphics[width=1\linewidth]{_main_files/figure-latex/relationship-x-y-1} \caption{Relationship between Independent Variables and Price}\label{fig:relationship-x-y}
\end{figure}

\begin{figure}[!h]
\includegraphics[width=1\linewidth,height=0.8\textheight]{_main_files/figure-latex/coplot-code-designer-name-1} \caption{Coplot of Size and Price Split by Designer and Name}\label{fig:coplot-code-designer-name}
\end{figure}

\begin{verbatim}
## 
##  Missing rows: 1, 2, 3, 8, 10, 16, 18, 21, 24, 27, 29, 30, 31, 33, 34, 35, 38, 39, 40, 42, 43, 44, 46, 48, 50, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 69, 70, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 115, 116, 117, 118, 119, 121, 123, 125, 126, 128, 129, 131, 132, 133, 135, 136, 137, 139, 140, 141, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 179, 183, 184, 185, 186, 188, 189, 192, 193, 194, 197, 198, 199, 201, 204, 205, 206, 207, 208, 209, 217, 218, 219, 220, 223, 224, 225, 226, 227, 229, 232, 233, 234, 237, 243, 244, 245, 246, 247, 250, 253, 255, 256, 257, 258, 259, 260, 261, 262, 265, 267, 268, 269, 271, 275, 276, 277, 279, 282, 284, 286, 291, 293, 294, 295, 296, 297, 300, 301, 307, 308, 311, 312, 315, 317, 319, 321, 322, 325, 326, 327, 328, 330, 331, 332, 334, 336, 337, 338, 340, 343, 347, 349, 357, 361, 364, 366, 370, 372, 373, 374, 375, 376, 377, 380, 383, 384, 387, 389, 390, 391, 392, 393, 397, 398, 401, 402, 403, 404, 407, 408, 410, 414, 415, 416, 418, 419, 420, 421, 422, 425, 427, 428, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 460, 461, 463, 465, 467, 471, 473, 474, 475, 476, 478, 481, 482, 483, 484, 489, 490, 491, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 508, 512, 513, 514, 515, 517, 518, 520, 521, 522, 524, 525, 526, 527, 528, 529, 530, 531, 533, 535, 538, 539, 540, 542, 543, 549, 551, 552, 553, 554, 556, 557, 558, 559, 560, 561, 562, 563, 564, 567, 568, 569, 570, 572, 573, 575, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 592, 593, 594, 595, 597, 598, 601, 602, 604, 605, 607, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 628, 629, 630, 632, 633, 634, 635, 638, 639, 640, 641, 644, 645, 647, 652, 653, 654, 655, 656, 657, 658, 661, 662, 663, 664, 666, 667, 668, 669, 671, 673, 675, 677, 680, 681, 685, 686, 691, 692, 693, 694, 696, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 715, 717, 718, 720, 722, 724, 725, 726, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 739, 745, 747, 748, 749, 750, 751, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 779, 780, 781, 783, 785, 787, 789, 790, 791, 793, 794, 795, 796, 797, 798, 802, 804, 805, 806, 808, 811, 813, 816, 825, 830, 831, 836, 844, 850, 853, 854, 856, 857, 859, 860, 866, 867, 868, 870, 875, 877, 880, 883, 884, 885, 886, 887, 890, 896, 897, 898, 899, 900, 903, 907, 908, 909, 910, 911, 914, 915, 916, 917, 919, 921, 922, 923, 924, 925, 926, 927, 929, 931, 933, 936, 938, 939, 944, 945, 948, 949, 950, 951, 953, 954, 956, 960, 962, 963, 964, 965, 966, 967, 968, 969, 971, 972, 973, 977, 980, 981, 982, 983, 984, 985, 986, 988, 989, 990, 992, 993, 994, 996, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1013, 1014, 1018, 1019, 1020, 1021, 1022, 1024, 1027, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1045, 1047, 1048, 1049, 1051, 1053, 1054, 1055, 1057, 1062, 1063, 1066, 1067, 1069, 1070, 1071, 1072, 1073, 1075, 1076, 1077, 1083, 1084, 1085, 1086, 1087, 1089, 1090, 1092, 1094, 1096, 1098, 1099, 1100, 1101, 1102, 1103, 1105, 1107, 1111, 1112, 1114, 1116, 1118, 1119, 1122, 1123, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1136, 1138, 1139, 1140, 1142, 1144, 1145, 1149, 1150, 1153, 1155, 1157, 1158, 1159, 1164, 1165, 1166, 1167, 1169, 1171, 1173, 1175, 1176, 1178, 1179, 1182, 1186, 1187, 1189, 1192, 1193, 1194, 1197, 1198, 1199, 1202, 1204, 1206, 1207, 1210, 1214, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1226, 1227, 1231, 1232, 1233, 1235, 1236, 1238, 1239, 1240, 1241, 1242, 1243, 1245, 1249, 1250, 1251, 1252, 1253, 1255, 1256, 1260, 1261, 1264, 1265, 1271, 1272, 1273, 1276, 1277, 1278, 1279, 1280, 1283, 1284, 1285, 1286, 1287, 1289, 1290, 1291, 1292, 1293, 1294, 1296, 1297, 1299, 1303, 1306, 1307, 1308, 1309, 1310, 1312, 1313, 1314, 1316, 1318, 1319, 1321, 1322, 1323, 1327, 1333, 1334, 1336, 1337, 1339, 1341, 1343, 1344, 1345, 1347, 1349, 1351, 1352, 1353, 1354, 1356, 1357, 1358, 1359, 1360, 1361, 1363, 1367, 1368, 1370, 1372, 1373, 1377, 1378, 1379, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1390, 1391, 1397, 1399, 1401, 1402, 1404, 1405, 1407, 1408, 1409, 1410, 1411, 1412, 1414, 1417, 1418, 1419, 1420, 1421, 1423, 1426, 1427, 1428, 1429, 1430, 1433, 1434, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1447, 1448, 1449, 1450, 1451, 1454, 1459, 1462, 1463, 1466, 1467, 1469, 1470, 1471, 1472, 1473, 1474, 1477, 1478, 1479, 1484, 1485, 1487, 1488, 1489, 1491, 1492, 1493, 1494, 1496, 1497, 1498, 1499, 1500, 1501, 1503, 1504, 1505, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1517, 1518, 1521, 1522, 1527, 1529, 1530, 1531, 1533, 1534, 1535, 1536, 1538, 1539, 1540, 1541, 1542, 1544, 1546, 1547, 1548, 1549, 1550, 1551, 1553, 1554, 1555, 1556, 1557, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1573, 1574, 1575, 1576, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1605, 1608, 1609, 1610, 1611, 1614, 1615, 1616, 1622, 1625, 1626, 1630, 1631, 1638, 1641, 1642, 1646, 1647, 1651, 1652, 1657, 1660, 1666, 1669, 1673, 1678, 1685, 1686, 1687, 1698, 1701, 1728, 1731, 1732, 1733, 1736, 1737, 1738, 1740, 1744, 1745, 1748, 1750, 1754, 1756, 1758, 1760, 1766, 1767, 1768, 1769, 1772, 1773, 1776, 1777, 1780, 1781, 1784, 1785, 1786, 1789, 1790, 1792, 1794, 1795, 1799, 1800, 1802, 1803, 1804, 1806, 1808, 1809, 1812, 1815, 1816, 1817, 1826, 1830, 1831, 1832, 1835, 1838, 1840, 1844, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1865, 1866, 1868, 1869, 1870, 1871, 1875, 1878, 1879, 1881, 1883, 1885, 1886, 1887, 1888, 1895, 1897, 1901, 1902, 1906, 1907, 1909, 1910, 1912, 1913, 1917, 1919, 1922, 1927, 1930, 1936, 1941, 1943, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1954, 1955, 1956, 1958, 1959, 1960, 1961, 1964, 1965, 1966, 1968, 1972, 1973, 1974, 1975, 1976, 1978, 1979, 1980, 1982, 1983, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2001, 2002, 2003, 2005, 2007, 2008, 2009, 2010, 2012, 2013, 2014, 2015, 2016, 2017, 2020, 2023, 2024, 2025, 2026, 2029, 2032, 2033, 2034, 2035, 2037, 2039, 2040, 2041, 2043, 2044, 2047, 2048, 2049, 2050, 2052, 2053, 2055, 2059, 2060, 2061, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2076, 2077, 2080, 2081, 2084, 2085, 2086, 2087, 2089, 2092, 2097, 2098, 2100, 2104, 2105, 2106, 2108, 2110, 2111, 2112, 2113, 2115, 2118, 2119, 2120, 2122, 2124, 2127, 2128, 2129, 2131, 2132, 2133, 2134, 2136, 2137, 2138, 2139, 2140, 2144, 2145, 2146, 2147, 2148, 2149, 2151, 2152, 2155, 2158, 2160, 2162, 2163, 2165, 2166, 2167, 2169, 2170, 2173, 2174, 2176, 2179, 2181, 2184, 2187, 2191, 2194, 2195, 2196, 2197, 2200, 2204, 2205, 2207, 2208, 2209, 2210, 2212, 2213, 2214, 2215, 2216, 2222, 2224, 2225, 2226, 2227, 2228, 2230, 2231, 2235, 2236, 2239, 2245, 2246, 2247, 2248, 2249, 2253, 2255, 2256, 2261, 2265, 2266, 2272, 2273, 2276, 2280, 2286, 2287, 2293, 2294, 2295, 2297, 2298, 2300, 2301, 2302, 2303, 2309, 2310, 2311, 2316, 2317, 2318, 2319, 2322, 2323, 2324, 2325, 2328, 2329, 2330, 2331, 2332, 2333, 2336, 2337, 2339, 2340, 2343, 2344, 2347, 2348, 2352, 2354, 2355, 2358, 2360, 2361, 2362, 2363, 2364, 2366, 2368, 2369, 2370, 2371, 2374, 2379, 2380, 2381, 2384, 2390, 2394, 2395, 2396, 2397, 2402, 2404, 2406, 2407, 2408, 2409, 2411, 2413, 2414, 2415, 2416, 2417, 2418, 2420, 2421, 2424, 2425, 2427, 2428, 2429, 2431, 2434, 2441, 2442, 2445, 2448, 2449, 2450, 2452, 2453, 2454, 2456, 2457, 2458, 2460, 2463, 2466, 2467, 2468, 2470, 2472, 2473, 2474, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2485, 2489, 2490, 2494, 2495, 2499, 2500, 2501, 2502, 2503, 2505, 2507, 2508, 2510, 2511, 2512, 2514, 2515, 2517, 2519, 2520, 2522, 2524, 2525, 2526, 2527, 2528, 2530, 2531, 2533, 2534, 2535, 2537, 2542, 2546, 2548, 2549, 2550, 2552, 2554, 2555, 2557, 2561, 2562, 2563, 2566, 2567, 2568, 2570, 2571, 2573, 2574, 2575, 2579, 2580, 2584, 2587, 2588, 2591, 2592, 2593, 2594, 2598, 2603, 2604, 2605, 2607, 2608, 2609, 2610, 2611, 2612, 2615, 2617, 2618, 2621, 2623, 2627, 2629, 2630, 2631, 2632, 2633, 2635, 2637, 2639, 2641, 2642, 2643, 2644, 2645, 2647, 2649, 2650, 2652, 2654, 2655, 2656, 2657, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2673, 2676, 2677, 2678, 2679, 2680, 2682, 2683, 2685, 2687, 2688, 2689, 2690, 2692, 2693, 2694, 2695, 2696, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2712, 2714, 2715, 2716, 2718, 2720, 2721, 2722, 2723, 2725, 2726, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2750, 2755, 2756, 2757, 2758, 2760, 2761, 2762, 2765, 2766, 2767, 2768, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2782, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2796, 2797, 2798, 2800, 2801, 2802, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2823, 2824, 2825, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2841, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2853, 2854, 2855, 2856, 2857, 2859, 2860, 2861, 2863, 2864, 2865, 2866, 2867, 2869, 2870, 2871, 2872, 2874, 2876, 2877, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2906, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2919, 2920, 2921, 2922, 2923, 2924, 2926, 2928, 2929, 2930, 2931, 2932, 2934, 2935, 2936, 2937, 2939, 2940, 2941, 2942, 2944, 2945, 2946, 2947, 2948, 2950, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3042, 3043, 3044, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3064, 3066, 3068, 3069, 3070, 3071, 3072, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3193, 3194, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3238, 3239, 3240, 3242, 3243, 3244, 3245, 3246, 3247, 3249, 3251, 3252, 3253, 3254, 3256, 3258, 3259, 3260, 3261, 3262, 3265, 3266, 3270, 3271, 3273, 3274, 3275, 3279, 3283, 3285, 3287, 3291, 3292, 3293, 3298, 3299, 3300, 3302, 3303, 3304, 3306, 3311, 3312, 3315, 3316, 3317, 3320, 3321, 3323, 3325, 3326, 3328, 3330, 3331, 3332, 3333, 3335, 3339, 3341, 3342, 3343, 3346, 3347, 3348, 3349, 3350, 3351, 3353, 3355, 3356, 3357, 3359, 3360, 3361, 3362, 3363, 3364, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3461, 3465, 3467, 3468, 3475, 3478, 3500, 3502, 3503, 3505, 3509, 3515, 3519, 3523, 3525, 3528, 3532, 3538, 3541, 3543, 3547, 3554, 3556, 3558, 3559, 3566, 3567, 3573, 3577, 3586, 3589, 3593, 3597, 3601, 3602, 3605, 3608, 3615, 3621, 3624, 3627, 3629, 3637, 3638, 3668, 3672, 3674, 3677, 3678, 3681, 3688, 3690, 3691, 3692, 3693, 3694
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coplot}\NormalTok{(size_m3 }\OperatorTok{~}\StringTok{ }\NormalTok{price_eur }\OperatorTok{|}\StringTok{ }\KeywordTok{fct_lump_n}\NormalTok{(category, }\DecValTok{5}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{         }\KeywordTok{fct_lump_n}\NormalTok{(name, }\DecValTok{5}\NormalTok{), }\DataTypeTok{data =} \KeywordTok{drop_na}\NormalTok{(tidy_ikea), }\DataTypeTok{ylab =} \StringTok{"Size in m^3"}\NormalTok{,}
       \DataTypeTok{xlab =} \StringTok{"Price in Euro"}\NormalTok{, }\DataTypeTok{panel =} \ControlFlowTok{function}\NormalTok{(x, y, ...) \{}
\NormalTok{         tmp <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x, }\DataTypeTok{na.action =}\NormalTok{ na.omit)}
         \KeywordTok{abline}\NormalTok{(tmp)}
         \KeywordTok{points}\NormalTok{(x, y )\})}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coplot}\NormalTok{(size_m3 }\OperatorTok{~}\StringTok{ }\NormalTok{price_eur }\OperatorTok{|}\StringTok{ }\KeywordTok{fct_lump_n}\NormalTok{(designer, }\DecValTok{5}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{         }\KeywordTok{fct_lump_n}\NormalTok{(category, }\DecValTok{5}\NormalTok{), }\DataTypeTok{data =} \KeywordTok{drop_na}\NormalTok{(tidy_ikea), }
       \DataTypeTok{ylab =} \StringTok{"Size in m^3"}\NormalTok{,}
       \DataTypeTok{xlab =} \StringTok{"Price in €"}\NormalTok{, }\DataTypeTok{panel =} \ControlFlowTok{function}\NormalTok{(x, y, ...) \{}
\NormalTok{         tmp <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x, }\DataTypeTok{na.action =}\NormalTok{ na.omit)}
         \KeywordTok{abline}\NormalTok{(tmp)}
         \KeywordTok{points}\NormalTok{(x, y )\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{bibliography}{%
\chapter{Bibliography}\label{bibliography}}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-Breiman2001}{}%
Breiman, Leo. ``Random Forests.'' \emph{Machine Learning}, no. 45 (2001). \url{https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf}.

\leavevmode\hypertarget{ref-Groemping2009}{}%
Grömping, Ulrike. ``Variable Importance Assessment in Regression: Linear Regression versus Random Forest.'' \emph{The American Statistician}, nos. Vol. 63, No. 4 (2009): 308--19. \url{https://prof.beuth-hochschule.de/fileadmin/prof/groemp/downloads/tast_2E2009_2E08199.pdf}.

\leavevmode\hypertarget{ref-Liaw2002}{}%
Liaw, Andy, and Matthew Wiener. ``Classification and Regression by randomForest.'' \emph{R News}, nos. Vol. 2/3, December 2002 (2001). \url{https://www.researchgate.net/publication/228451484_Classification_and_Regression_by_RandomForest}.

\leavevmode\hypertarget{ref-Hadley2017}{}%
Wickham. \emph{R for Data Science}. O'Reilly Media, 2017.

\leavevmode\hypertarget{ref-Zuur2010}{}%
Zuur. ``A protocol for data exploration to avoid common statistical problems.'' \emph{British Ecological Society}, 2010. doi:\href{https://doi.org/10.1145/1738826.1738829}{10.1145/1738826.1738829}.


%%%%% REFERENCES

% JEM: Quote for the top of references (just like a chapter quote if you're using them).  Comment to skip.
% \begin{savequote}[8cm]
% The first kind of intellectual and artistic personality belongs to the hedgehogs, the second to the foxes \dots
%   \qauthor{--- Sir Isaiah Berlin \cite{berlin_hedgehog_2013}}
% \end{savequote}

\setlength{\baselineskip}{0pt} % JEM: Single-space References

{\renewcommand*\MakeUppercase[1]{#1}%
\printbibliography[heading=bibintoc,title={\bibtitle}]}

\end{document}
